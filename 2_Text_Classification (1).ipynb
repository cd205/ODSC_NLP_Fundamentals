{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "2 Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ajZb5wFGupT"
      },
      "source": [
        "# Lesson 2 Text Classification \n",
        "\n",
        "Theory: We will learn how it’s possible to represent text and how a classifier can use this representation. We will use TF-Idf and experiment with a couple of supervised learning models.\n",
        "\n",
        "Exercise: Build an NLP pipeline to perform classification.\n",
        "We will need to clean the text, transform it into something readable by an algorithm, and finally classify it.\n",
        "\n",
        "Outcome: You will be able to solve a text classification problem end to end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FElbzWi7GupU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688f8f24-4fb5-4533-ae81-ea956b2d6e47"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_Q851xGupZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4adcf52-78c9-4e2b-b511-945d9854c65a"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "\n",
        "ng_train = fetch_20newsgroups(subset='train', \n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=11)\n",
        "\n",
        "ng_test = fetch_20newsgroups(subset='test',\n",
        "                                 categories=categories,\n",
        "                                 shuffle=True,\n",
        "                                 random_state=11)\n",
        "\n",
        "\n",
        "ng_test.target\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 1, ..., 1, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-hEcnHRGupc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiUqfREXGupf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7f6e86-c964-4993-c2ba-782876721e4c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "spacy.load('en')\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "ENGLISH_STOP_WORDS = []\n",
        "parser = English()\n",
        "\n",
        "\n",
        "STOPLIST = set(set(stopwords.words('english')).union( set(ENGLISH_STOP_WORDS)))\n",
        "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
        "\n",
        "class CleanTextTransformer(TransformerMixin):\n",
        "    \n",
        "    def _clean_text(self, text):\n",
        "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "        text = text.lower()\n",
        "        return text\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [self._clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def get_params(self, deep=True):\n",
        "            return {}\n",
        "    \n",
        "\n",
        "\n",
        "def tokenize_text(sample):\n",
        "    '''\n",
        "    Here we are using SpaCy\n",
        "    '''\n",
        "    tokens = parser(sample)\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        lemmas.append(token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_)\n",
        "    tokens = lemmas\n",
        "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
        "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
        "    return tokens\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb21gbcoGupi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53410c16-a721-4575-e5df-d9c1d26a4719"
      },
      "source": [
        "\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize_text, \n",
        "                             ngram_range=(1,1))\n",
        "clf = LinearSVC()\n",
        "\n",
        "pipe = Pipeline([('cleanText', CleanTextTransformer()),\n",
        "                 ('vectorizer', vectorizer), \n",
        "                 ('clf', clf)])\n",
        "# data\n",
        "X_train = ng_train.data\n",
        "y_train = ng_train.target\n",
        "X_test = ng_test.data\n",
        "y_test = ng_test.target\n",
        "# train\n",
        "pipe.fit(X_train, y_train)\n",
        "# test\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred,\n",
        "    target_names=ng_train.target_names))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.90      0.77      0.83       319\n",
            "         comp.graphics       0.87      0.94      0.90       389\n",
            "               sci.med       0.92      0.84      0.88       396\n",
            "soc.religion.christian       0.85      0.94      0.89       398\n",
            "\n",
            "              accuracy                           0.88      1502\n",
            "             macro avg       0.88      0.88      0.88      1502\n",
            "          weighted avg       0.88      0.88      0.88      1502\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1-YlV7dGupp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b24c86b-76aa-4923-edfd-e58d37b82cb8"
      },
      "source": [
        "y_test, y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2, 1, 1, ..., 1, 2, 0]), array([2, 1, 1, ..., 1, 2, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBG1HOEGCS53"
      },
      "source": [
        "# Excercise\n",
        "\n",
        "Improve the above classifier, creating a new pipeline, add some transformations and check the performanfes of other classifiers of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYBefeDLplIk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}