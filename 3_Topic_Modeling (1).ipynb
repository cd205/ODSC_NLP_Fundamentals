{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "3 Topic Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MVmh4zwD7rzn",
        "mxEqAWKtITyL",
        "4YTg3r-GEAY9",
        "XZ8Sji6yKN16"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVmh4zwD7rzn"
      },
      "source": [
        "# Lesson 3 Topic Modeling\n",
        "\n",
        "\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. \n",
        "\n",
        "\n",
        "In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "\n",
        "- Dimensionality Reduction, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "\n",
        "\n",
        "- Unsupervised Learning, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "\n",
        "\n",
        "Tagging, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "In this article, we’ll take a closer look at LDA, and implement our first topic model using the sklearn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIiiN3ml8TeT"
      },
      "source": [
        "!pip install pyldavis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dq-qFrO-M-q"
      },
      "source": [
        "# Load the data\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# categories = ['alt.atheism', \n",
        "#               'comp.graphics', 'sci.med']\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', \n",
        "                                  # categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=11)\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "                                #  categories=categories,\n",
        "                                 shuffle=True,\n",
        "                                 random_state=11)\n",
        "\n",
        "\n",
        "categories = twenty_test.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnTeVWtY7rzp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "a28ad1a3-113c-4aa8-9da7-1174c0b83099"
      },
      "source": [
        "def display_topics(H, W, feature_names, no_top_words, n_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:n_top_documents]\n",
        "\n",
        "            \n",
        "sample_text = twenty_train.data[1]\n",
        "sample_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From: chrism@cirrus.com (Chris Metcalfe)\\nSubject: Nazi Eugenic Theories Circulated by CPR => (unconventianal peace)\\nOrganization: Cirrus Logic Inc.\\nLines: 85\\n\\nNow we have strong evidence of where the CPR really stands.\\nUnbelievable and disgusting.  It only proves that we must\\nnever forget...\\n\\n\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\nIn article <1483500348@igc.apc.org> cpr@igc.apc.org (Center for Policy Research) writes:\\n>\\n>From: Center for Policy Research <cpr>\\n>Subject: Unconventional peace proposal\\n>\\n>\\n>A unconventional proposal for peace in the Middle-East.\\n\\nNot so unconventional.  Eugenic solutions to the Jewish Problem\\nhave been suggested by Northern Europeans in the past.\\n\\n  Eugenics: a science that deals with the improvement (as by\\n  control of human mating) of hereditory qualities of race\\n  or breed.  -- Webster\\'s Ninth Collegiate Dictionary.\\n\\n>5.      The emergence of a considerable number of \\'mixed\\'\\n>marriages in Israel/Palestine, all of whom would have relatives on\\n>\\'both sides\\' of the divide, would make the conflict lose its\\n>ethnical and unsoluble core and strengthen the emergence of a\\n>truly civil society. The existence of a strong \\'mixed\\' stock of\\n>people would also help the integration of Israeli society into the\\n>Middle-East in a graceful manner.\\n\\nThis is nothing more than Feisal Husseini\\'s statement that the\\nZionist entity must be disolved by forcing it to \"engage\" the\\nsurrounding \"normal\" Arab society.\\n\\n\"a strong mixed stock\", \"integration of Israeli society into\\nthe Middle East in a graceful manner,\" these are the phrases\\nof Nazi racial engineering pure and simple.  As if Israeli\\nsociety has no right to exist per se!\\n\\n>3.      Fundamentalist Jews would certainly object to the use of\\n>financial incentives to encourage \\'mixed marriages\\'. From their\\n>point of view, the continued existence of a specific Jewish People\\n>overrides any other consideration, be it human love, peace of\\n>human rights.  The President of the World Jewish Congress, Edgar\\n>Bronfman, reflected this view a few years ago in an interview he\\n>gave to Der Spiegel, a German magazine. He called the increasing\\n>assimilation of Jews in the world a <calamity>, comparable in its\\n>effects only with the Holocaust. This objection has no merit\\n>either because it does not fulfill the first two assumptions (see\\n>above)\\n\\n\"the continued existance of a specific Jewish People overrides\\nany other consideration, be it human love, peace of human\\nrights.\"  Disolve the Jewish People and protect human values\\nsuch as love and peace; yes ve have heard this before Her Himmler.\\nNotice how the source of the problem seems to be accruing to\\nthe Jews in this analysis.  Ya, Der Spiegal ist a gut sourcen...\\n\\n>5.      It may objected that such a Fund would need great sums to\\n>bring about substantial demographic changes. This objection has\\n>merits. However, it must be remembered that huge sums, more than\\n>$3 billion, are expended each year by the United States government\\n>and by U.S. organizations to maintain an elusive peace in the\\n>Middle-East through armaments. A mere fraction of these sums would\\n>suffice to launch the above proposal and create a more favorable\\n>climate towards the existence of \\'mixed\\' marriages in\\n>Israel/Palestine, thus encouraging the emergence of a\\n>non-segregated society in that worn-torn land.\\n\\nNice attempt to mix in a slam against U.S. aid to Israel.\\n\\n>I would be thankful for critical comments to the above proposal as\\n>well for any dissemination of this proposal for meaningful\\n>discussion and enrichment.\\n>\\n>Elias Davidsson Post Box 1760 121 Reykjavik, ICELAND\\n\\nCritical comment: you can take the Nazi flag and Holocaust photos\\noff of your bedroom wall, Elias; you\\'ll never succeed.\\n\\n-- Chris Metcalfe\\n\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\nNow we\\'ll find out where you fans really stand...\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypDHGJZMF-5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3f27db-70de-45be-b8d4-07b7817ef4d8"
      },
      "source": [
        "dir(LatentDirichletAllocation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_approx_bound',\n",
              " '_check_feature_names',\n",
              " '_check_n_features',\n",
              " '_check_non_neg_array',\n",
              " '_check_params',\n",
              " '_e_step',\n",
              " '_em_step',\n",
              " '_get_param_names',\n",
              " '_get_tags',\n",
              " '_init_latent_vars',\n",
              " '_more_tags',\n",
              " '_perplexity_precomp_distr',\n",
              " '_repr_html_',\n",
              " '_repr_html_inner',\n",
              " '_repr_mimebundle_',\n",
              " '_unnormalized_transform',\n",
              " '_validate_data',\n",
              " 'fit',\n",
              " 'fit_transform',\n",
              " 'get_params',\n",
              " 'partial_fit',\n",
              " 'perplexity',\n",
              " 'score',\n",
              " 'set_params',\n",
              " 'transform']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6nT-Ctr7rzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738711d3-f741-44e4-dec5-a09ba03a7b47"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "docs = twenty_train.data\n",
        "\n",
        "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
        "tf_vectorizer = CountVectorizer( stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(docs)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "n_topics = len(categories)\n",
        "n_topics = 20\n",
        "n_top_words = 5\n",
        "n_top_documents = 5\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "display_topics(lda_H, lda_W, tf_feature_names, n_top_words, n_top_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA Topics\n",
            "Topic 0:\n",
            "nasa space 00 gov jpl\n",
            "Topic 1:\n",
            "__ ___ mit lcs fr\n",
            "Topic 2:\n",
            "gerard alleg de7 uccxkvb dps\n",
            "Topic 3:\n",
            "cx w7 c_ mv uw\n",
            "Topic 4:\n",
            "db mov nwu een acns\n",
            "Topic 5:\n",
            "uchicago frank objective morality midway\n",
            "Topic 6:\n",
            "temple ge dane keele ocis\n",
            "Topic 7:\n",
            "edu subject lines organization com\n",
            "Topic 8:\n",
            "toronto henry udel spencer zoo\n",
            "Topic 9:\n",
            "uk ac liverpool liv archbishop\n",
            "Topic 10:\n",
            "ax max g9v b8f a86\n",
            "Topic 11:\n",
            "nfotis plplot virginia plot roy\n",
            "Topic 12:\n",
            "key file use com chip\n",
            "Topic 13:\n",
            "edu people com writes subject\n",
            "Topic 14:\n",
            "adobe smokeless nichols sherri snichols\n",
            "Topic 15:\n",
            "drive 55 16 ide entry\n",
            "Topic 16:\n",
            "radar ncr detector detectors waterloo\n",
            "Topic 17:\n",
            "stratus sw wpi cdt atf\n",
            "Topic 18:\n",
            "ncsu harris eos hernlem uoregon\n",
            "Topic 19:\n",
            "georgia ai uga michael athens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxEqAWKtITyL"
      },
      "source": [
        "## Removing some data\n",
        "\n",
        "Now let's remove some of the metadata to see if there is any improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfhiPmG7rz5"
      },
      "source": [
        "remove_info = ('headers', 'footers', 'quotes'),\n",
        "    \n",
        "twenty_train = fetch_20newsgroups(subset='train', \n",
        "                                  remove=remove_info,\n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=11)\n",
        "\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "                                 remove=remove_info,\n",
        "                                 categories=categories,\n",
        "                                 shuffle=True,\n",
        "                                 random_state=11)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhMI2467r0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e9f031-cc8a-4d12-f0a8-9bdd1d3ae47d"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "docs = twenty_train.data\n",
        "\n",
        "n_topics = 20\n",
        "n_top_words = 11\n",
        "n_top_documents = 5\n",
        "n_features = 1000\n",
        "\n",
        "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
        "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=5, \n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(docs)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "n_topics = len(categories)\n",
        "\n",
        "\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, \n",
        "                                      learning_method='online', \n",
        "                                      learning_offset=50.,random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "display_topics(lda_H, lda_W, tf_feature_names, n_top_words, n_top_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA Topics\n",
            "Topic 0:\n",
            "game team year play games ca edu season hockey win players\n",
            "Topic 1:\n",
            "car uiuc time just went bike left cars didn cso like\n",
            "Topic 2:\n",
            "cs edu information science computer list berkeley colorado faq sci pitt\n",
            "Topic 3:\n",
            "file window program mit files image available server ftp use code\n",
            "Topic 4:\n",
            "key uk chip encryption clipper ac keys security government algorithm privacy\n",
            "Topic 5:\n",
            "card __ speed ___ ca use drivers driver bus performance tom\n",
            "Topic 6:\n",
            "com netcom edu writes article jim au virginia brian fbi david\n",
            "Topic 7:\n",
            "gun government law state people rights guns right control states american\n",
            "Topic 8:\n",
            "bit using use work problem memory video com mouse time disk\n",
            "Topic 9:\n",
            "00 10 16 scsi 15 drive 25 apr 20 14 11\n",
            "Topic 10:\n",
            "windows dos graphics ibm ms pc os color washington software purdue\n",
            "Topic 11:\n",
            "israel jews israeli armenian turkish people armenians jewish war men said\n",
            "Topic 12:\n",
            "com posting host nntp access writes ca hp distribution reply article\n",
            "Topic 13:\n",
            "ax max g9v b8f a86 145 pl 1d9 0t 34u 1t\n",
            "Topic 14:\n",
            "god jesus people does christian believe say think bible christians don\n",
            "Topic 15:\n",
            "edu university thanks posting host nntp ca mail state help know\n",
            "Topic 16:\n",
            "edu writes article posting host nntp com university cc just cs\n",
            "Topic 17:\n",
            "don people like just think know good make want going ve\n",
            "Topic 18:\n",
            "space gov nasa president center research program national clinton earth mr\n",
            "Topic 19:\n",
            "edu apple sale university host mac posting nntp cwru newsreader tin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx35aM0x7r0K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "aa40394e-e08d-44e1-c571-06938ef99097"
      },
      "source": [
        "ng_train = fetch_20newsgroups(subset='train', \n",
        "                                  remove=remove_info,\n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=11)\n",
        "\n",
        "ng_train.data[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From: behanna@syl.nj.nec.com (Chris BeHanna)\\nSubject: Re: Should liability insurance be required?\\nOrganization: NEC Systems Laboratory, Inc.\\nDistribution: usa\\nLines: 32\\n\\nIn article <tcora-140493155620@b329-gator-3.pica.army.mil> tcora@pica.army.mil (Tom Coradeschi) writes:\\n>In article <1993Apr14.125209.21247@walter.bellcore.com>,\\n>fist@iscp.bellcore.com (Richard Pierson) wrote:\\n>> \\n>> Lets get this \"No Fault\" stuff straight, I lived in NJ\\n>> when NF started, my rates went up, ALOT. Moved to PA\\n>> and my rates went down ALOT, the NF came to PA and it\\n>> was a different story. If you are sitting in a parking\\n>> lot having lunch or whatever and someone wacks you guess\\n>> whose insurance pays for it ? give up ?  YOURS.\\n>\\n>BZZZT! If it is the other driver\\'s fault, your insurance co pays you, less\\n>deductible, then recoups the total cost from the other guy/gal\\'s company\\n>(there\\'s a fancy word for it, which escapes me right now), and pays you the\\n>deductible. Or: you can go to the other guy/gal\\'s company right off - just\\n>takes longer to get your cash (as opposed to State Farm, who cut me a check\\n>today, on the spot, for the damage to my wife\\'s cage).\\n\\n\\tThe word is \"subrogation.\"  Seems to me, if you\\'re willing to wait\\nfor the money from scumbag\\'s insurance, that you save having to pay the\\ndeductible.  However, if scumbag\\'s insurance is Scum insurance, then you may\\nhave to pay the deductible to get your insurance co.\\'s pack of rabid, large-\\nfanged lawyers to recover the damages from Scum insurance\\'s lawyers.\\n\\n\\tSad, but true.  Call it job security for lawyers.\\n\\nLater,\\n-- \\nChris BeHanna\\tDoD# 114          1983 H-D FXWG Wide Glide - Jubilee\\'s Red Lady\\nbehanna@syl.nj.nec.com\\t          1975 CB360T - Baby Bike\\nDisclaimer:  Now why would NEC\\t  1991 ZX-11 - needs a name\\nagree with any of this anyway?    I was raised by a pack of wild corn dogs.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbQ7aEXW7r0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7565a8-89ce-4445-d96f-026f92d5090d"
      },
      "source": [
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda_model, tf_feature_names, n_top_words)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #0: game team year play games ca edu season hockey win players\n",
            "Topic #1: car uiuc time just went bike left cars didn cso like\n",
            "Topic #2: cs edu information science computer list berkeley colorado faq sci pitt\n",
            "Topic #3: file window program mit files image available server ftp use code\n",
            "Topic #4: key uk chip encryption clipper ac keys security government algorithm privacy\n",
            "Topic #5: card __ speed ___ ca use drivers driver bus performance tom\n",
            "Topic #6: com netcom edu writes article jim au virginia brian fbi david\n",
            "Topic #7: gun government law state people rights guns right control states american\n",
            "Topic #8: bit using use work problem memory video com mouse time disk\n",
            "Topic #9: 00 10 16 scsi 15 drive 25 apr 20 14 11\n",
            "Topic #10: windows dos graphics ibm ms pc os color washington software purdue\n",
            "Topic #11: israel jews israeli armenian turkish people armenians jewish war men said\n",
            "Topic #12: com posting host nntp access writes ca hp distribution reply article\n",
            "Topic #13: ax max g9v b8f a86 145 pl 1d9 0t 34u 1t\n",
            "Topic #14: god jesus people does christian believe say think bible christians don\n",
            "Topic #15: edu university thanks posting host nntp ca mail state help know\n",
            "Topic #16: edu writes article posting host nntp com university cc just cs\n",
            "Topic #17: don people like just think know good make want going ve\n",
            "Topic #18: space gov nasa president center research program national clinton earth mr\n",
            "Topic #19: edu apple sale university host mac posting nntp cwru newsreader tin\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ_Pawo07r0U"
      },
      "source": [
        "transformed = lda_model.transform(tf)\n",
        "doc_topic_dist_unnormalized = np.matrix(transformed)\n",
        "\n",
        "# normalize the distribution (only needed if you want to work with the probabilities)\n",
        "doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)\n",
        "\n",
        "res = doc_topic_dist.argmax(axis=1).ravel().tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mco34Qk7r0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573830f1-01a6-4635-9843-c5075fcc43ae"
      },
      "source": [
        "\n",
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "data, _ = fetch_20newsgroups(shuffle=True, \n",
        "                             random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "\n",
        "# Use tf-idf features for NMF.\n",
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "\n",
        "# Use tf (raw term count) features for LDA.\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print()\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting tf-idf features for NMF...\n",
            "Extracting tf features for LDA...\n",
            "\n",
            "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py:1425: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py:294: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
            "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
            "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
            "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
            "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
            "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
            "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
            "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
            "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
            "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
            "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
            "\n",
            "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 4.372s.\n",
            "\n",
            "Topics in LDA model:\n",
            "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
            "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
            "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
            "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
            "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
            "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
            "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
            "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
            "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
            "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YTg3r-GEAY9"
      },
      "source": [
        "## LDAvis\n",
        "\n",
        "A better way to explore the LDA topics is to use pyldavis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad95DwbU7r0r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8426d908-fc75-48a8-bebb-8c400a49bb35"
      },
      "source": [
        "from __future__ import division\n",
        "!pip install pyLDAvis\n",
        "import pandas as pd\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "docs_raw = twenty_train.data\n",
        "\n",
        "\n",
        "dtm_tf = tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=4ba403cbac70c13db47162eba7f4b10d096aebf42cec4a6f91e7243987ab59eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.4 pandas-1.3.4 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9BwMlHo7r0u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "a9e13515-28d7-4e00-a980-5347f92d0030"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda_model, dtm_tf, tf_vectorizer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0502c5acdd13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mdoc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_check\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m__num_dist_rows__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not all rows (distributions) in topic_term_dists sum to 1.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m__num_dist_rows__\u001b[0;34m(array, ndigits)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__num_dist_rows__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;31m#  to avoid constructing two potentially large/sparse DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     join_columns, _, _ = left.columns.join(\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_indexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# TODO: same for tuples?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mevaluating\u001b[0m \u001b[0mop\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mIf\u001b[0m \u001b[0mnative\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mtry\u001b[0m \u001b[0mcoercion\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuncType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/computation/check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numexpr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"warn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: import_optional_dependency() got an unexpected keyword argument 'errors'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBwyUl0m7r0x"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda_model, dtm_tf, tf_vectorizer, mds='tsne')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ8Sji6yKN16"
      },
      "source": [
        "## Excercise\n",
        "\n",
        "Explore different parameters for the LDA model and visualize the results. Create a new pipline and experiment with HashVectorizer instehad of CounterVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjWWEOF7r00"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIBzQ4FGw4qX"
      },
      "source": [
        "\n",
        "# Non-Negative Matrix Factorization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b6VyzGIw7bt"
      },
      "source": [
        "# Importing Necessary packages\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M3NLrP2xB3p",
        "outputId": "39f5afb8-123f-4fb8-bf1e-aa97fdf7d86a"
      },
      "source": [
        "# Let's check the first 3 articles\n",
        "text_data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n",
        "text_data[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.',\n",
              " \"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\",\n",
              " 'well folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6NFSvmxLdP",
        "outputId": "cce00ca1-c2dd-432f-8b59-3df8e4c3b51a"
      },
      "source": [
        "# converting the given text term-document matrix\n",
        " \n",
        "vectorizer = TfidfVectorizer(max_features=1500, min_df=10, stop_words='english')\n",
        "X = vectorizer.fit_transform(text_data)\n",
        "words = np.array(vectorizer.get_feature_names())\n",
        "\n",
        "# The algorithm splits each term in the document and assigns weightage to each words.\n",
        "print(X)\n",
        "print(\"X = \", words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 829)\t0.13596515131134768\n",
            "  (0, 809)\t0.1439640091285723\n",
            "  (0, 707)\t0.16068505607893963\n",
            "  (0, 672)\t0.16927150728890597\n",
            "  (0, 1495)\t0.1274990882101728\n",
            "  (0, 506)\t0.19413995565094086\n",
            "  (0, 887)\t0.17648781190400797\n",
            "  (0, 757)\t0.09424560560725692\n",
            "  (0, 247)\t0.17513150125349702\n",
            "  (0, 1158)\t0.1651151431885443\n",
            "  (0, 1218)\t0.19781957502373113\n",
            "  (0, 128)\t0.190572546028195\n",
            "  (0, 1256)\t0.153503242191245\n",
            "  (0, 1118)\t0.12154002727766956\n",
            "  (0, 273)\t0.14279390121865662\n",
            "  (0, 484)\t0.1714763727922697\n",
            "  (0, 767)\t0.18711856186440218\n",
            "  (0, 808)\t0.18303366583393096\n",
            "  (0, 469)\t0.2009979730339519\n",
            "  (0, 411)\t0.14249215589040326\n",
            "  (0, 1191)\t0.17201525862610714\n",
            "  (0, 278)\t0.630558141606117\n",
            "  (0, 1472)\t0.1855076564575762\n",
            "  (1, 1355)\t0.12138696862814867\n",
            "  (1, 653)\t0.1728163048656526\n",
            "  :\t:\n",
            "  (11312, 1027)\t0.45507155319966874\n",
            "  (11312, 647)\t0.21811161764585577\n",
            "  (11312, 1302)\t0.2391477981479836\n",
            "  (11312, 1276)\t0.39611960235510485\n",
            "  (11312, 1100)\t0.1839292570975713\n",
            "  (11312, 926)\t0.2458009890045144\n",
            "  (11312, 1409)\t0.2006451645457405\n",
            "  (11312, 1486)\t0.183845539553728\n",
            "  (11312, 379)\t0.20769215501186927\n",
            "  (11312, 1026)\t0.16047640328575621\n",
            "  (11312, 1037)\t0.23285032689635052\n",
            "  (11312, 1082)\t0.23999986388716432\n",
            "  (11313, 244)\t0.27766069716692826\n",
            "  (11313, 1457)\t0.24327295967949422\n",
            "  (11313, 1219)\t0.26985268594168194\n",
            "  (11313, 1225)\t0.30171113023356894\n",
            "  (11313, 272)\t0.2725556981757495\n",
            "  (11313, 1394)\t0.238785899543691\n",
            "  (11313, 46)\t0.4263227148758932\n",
            "  (11313, 637)\t0.22561030228734125\n",
            "  (11313, 666)\t0.18286797664790702\n",
            "  (11313, 18)\t0.20991004117190362\n",
            "  (11313, 801)\t0.18133646100428719\n",
            "  (11313, 950)\t0.38841024980735567\n",
            "  (11313, 506)\t0.2732544408814576\n",
            "X =  ['00' '000' '01' ... 'york' 'young' 'zip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acceyCC-xYxY"
      },
      "source": [
        "\n",
        "Let's apply NMF to our data and view the topics generated. For simplicity, we will look at 10 topics that the model has generated. \n",
        "\n",
        "A commonly used method of optimization is the multiplicative update method. In this method, W and H are each updated iteratively according to the following rule:\n",
        "\n",
        "\n",
        "![img.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJ4AAAAdCAIAAADUw0JaAAAFGklEQVR42uxaMXKrOhQV76dwOsES5HTpsMuUcpfS9hJEunTgMiVKl1SYJWCvALME7DIVsARwmc5/JvePRiNjG4zivP+eT2VrQDog6dyrc7nZ7XboiotjNBqRLyRJUhQFY6wsyzAMq6rSNcTN9S1fHlVV2bbNOUcIFUVBKfU8DyFkWZbGUX5dX/TlURTFbDaD30mSUErht23bGkcxroL8s9vXsqxvmoLrrtUDz/MMwxiNRqvVCiGU5/lkMjEMw3GcPM/hGsdxTNMEHQakaXp8p3LO+/2+YRjirtVqBQMtFosTnHZXaAJCKIoi8TeOY4RQWZaiJYqiLMvkW1zXZYwd7xb6SdNUdBLHcRM+112rDRjjsizF3yRJIKwK7UUI9ft9+ZbNZiMC7ZFcmlIKsXk+nxNCRqNRI0Ly6hDDEEIopbDEsiwbj8cYY2BPKW24avTiN6e32+1s23ZdF34HQQDTLMj4vq88juu6CCHG2EnCaZrClcqmP6Eiyn9KKcZ4/zrgIWThp/A70xt/AVYbzBbGOAiCWiluC0KIbdutblEFWc7FlXaM8WAw+FnR00VvtVpxzj3P45yv12st3CzL2m63CKHlcgmaSQgpiqJWiluBcx4EwWazOZ06HUqjYOPDQpMB2gJL8gehhV4QBLIGlmUZRVETVTwJ3/cJISDFQmPG47EixWd0K1SBEHKmIPu+ryR14nXUvtPjoVEEHl3oTs913UNTGHyhC70oiuTgutvtGGOQW53XYZZl8rLIskxJwlsIcpIkhBDTNPflDiE0HA6bi0EYhidzvzPUuAs9zvl0Oj2UXjqOgxDqIs6EEMaY3D8hxPf9fcInsV6vJ5PJ3d2dkk5jjBlj8sm4qRtlmqZlWfuH6OVyiTFu7lzneT6dTnXFMC30qqoKwxCs2iNwHGc+n/8hpzElku3vd9ABSmlDHSjL0rZt7clqR3q+7zcRxiiKfvwUoAu/FFnbV9HNZlPbfkhJKKW+72vPpTvSK8tSEUbO+WAwAF9QgFIKA/0BuFHOD4ciWZOp5ZyDafL09NSKRBzHJ88G3ekpAKqz2UyOjqZpyo6SGr0M4385tWma1r4gUMImu9DzPIzx6+trFEXad213egrG4/FyuWSM7ZdijgSvtqNcoOR+ItZCxKrN/lsFWgiKhJCO5kttQO1Cr+FJLP6CLtplWYpxGWPiJNPxpNvuXAtHw/0MAsoObamA36uRZXd6aZo2ObaerMO0Tf1E7kYIEfwv43Ij4ZvUFvjO9maFfa/LOu5Oz3Xd40lyHMcnDQEYURQhoDihePfgVMgLDuK3xg6DICCEQCM8VBAEGGM4Sf83tYwxcVK0bVsWDfnDjrbLOQgCLW6UXnoQ6jraZ2fUZeM4PmLun9EhyIAiV6IaUV/50YizDbZvheu6yhk3yzLXdZu7jMobVJQDTOn9QY8svjM6hORDlitokZ/rb/zKAuZSQDb0tddlRUA5IvVndAjJh9KiFA/+xo9V+/1+Qxv2kFcM307keQ6eNsYYWhaLBURKuXqYfAFCY62D3apD2U6XnyIMQ/XK3RXtwz/k/2I/wbarVc5v6lDRcAjYSuJ9/TbqzF07n8+F3WFZVlEUYRhOJpMLdLher7fb7XQ6lbUB7JHrx6oaplYuL4LZpBhb39fhfnGz9uOTf15eXq6z1Qqfn58IoefnZ9Hy8fHx+Pj48PDw3R1WVfX+/u77/u3tba/XGw6H0PL29tbr9SzLur+/Fxf/GwAA//8vzEMJkMwO9AAAAABJRU5ErkJggg==)\n",
        "\n",
        "\n",
        "\n",
        "![img2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKIAAAAdCAIAAACmDCDsAAAFHUlEQVR42uxasXayTBNe86cw3cIlLOnSoWXKNVVK5BIwXTqkTAl2SbXuJaBXgF6CWqZCLgFSpvM/b+Y9++27IAFFj57DU+Wbb5F52Z1nnpnZ291uh1pcDwaDAfnBcrlMksRxnDRNOedZlpU8ddt+uCtClmWmaQZBgBBKkoRSOh6PEUK6rpc/eNN+uytCkiSe58Hfy+WSUgp/m6ZZ/mCnJe0rDWtd16vvXRvNJ0cQBIZhdDodIFuE0GKx6HQ6g8FgNpvJK6fTqWEYmqYFQQC5djqdappmGIZ4FrBarX6N4H+wa3F6RFEEewP/GYZhFEWFKwkhvu/LFowxY0xZ5rqu4zjVHWi3+UygP9jtdowxsd8K4jiWT4OwpGma/7UwDGtss+u6GGOIbEppHMfivBTa4R1gtyzrnF/qilzNY7VaIYQcx5HdU8AYU/iVMUYIUYjBdV34qX2UUBzNYRgihBSuKLHD5ytx93S4IlcLCdk0zZIFlmUBaQsQQlzXPf7Vf+pmQkhh2hYhomC73VqWZRjG+eXMSV1dLBabzSZNU13XKaW9Xq9ZIcYYe3p6ms1mw+GwcM1yufR9fzQaCYvneYKQjoJIAPlT4zhOob0waM6DE7nKGJM5ME3TMAxrsWI5hBsQryWsLqdhEG6NOIDgX5XPXqvVCphQUXRRFO1TECU6sxHmOZGrruvu2072gyPPpXzU4JgWqiegaMUxUG3H40/drGkaQujr60shEOAW+LICm82mLptxzpthnhO4GgSBbduDwaDw/wJ/rtfrA1xdr9fD4fD+/l5xCWPsOI5cB2dZFgSB7/tQKAsL5zxNU6W2Ppy0oZkiq4MwDEG2gHaVT3ddORPHcbnuqM0/zbmapmkVVq9VoV4mboS0kUMkTVOQLRjjJEnEoUMI1VJeWZbZts05b1aFNeUq5xySejkopYcF9AUBdts0TYxxXrYQQgrtFStF0zSbUjECDbpaKNnyPlcM+kvG30GkruubzQYqELloIYRAiCj2KvUDzFJeXl5qHbsoisqjsHFXZYDPnufJ2VrTNCXr/zP86XQuP5hv5bozy7L5fA4jTKUe5Zwr3fNyjMdjjPFkMgnDsNkCtHFXZViWNZ/PFSaHcVAJHV7+dYP/ohmEviKJwT6dTm3brvvi0WjU7/dt2/41QGvhFK4KFMra2uOgy7tucCOHQr4CAXuSJIdFZK/XY4zV5e0q0dyIq7ZtQwFTjvl8vq/iuprrBnLTPD8JgWIub687nGmwq9ysq67rlj8SRdGvsyCYJVBKQbvFcWxZljKocBwHY6xIOZhMgB3cYIxhjJVxZMVlon20dxAZhmFhu2efve7GNNUFO4WrkMaO7N8pvS3oU8o/K+r7wwbMFZdFUVTYpTjTvPlIPjjDiFMEiujquK5b/dwoHx3iW7RaoUl+8IC5+hx633WDM93shCblJd/j2W63QPuiPPM8r7rbopwDGeh53mQyEWUY51wuCuQ2LSgY2UIIUd5bcRlIlsKGT3uB9y/y963q9ubkql3TNNGVm81mkKoLt5kQIr+Xc55fXGXZYrFY/gCyuKoZ2+s7jcBxHOioiwxqmibouxIFl8+v0CA6bFl7F+zkgDEiY0zkS0qpZVklXdKKA+ZG5tDtBd7GBipJksj5EhpSJaORfH6VS966y8rxv7e3t3aTjsf39zdC6PX1VVg+Pz+fn58fHx8Lm1YfHx++79/d3XW73X6/D5b39/dut6vr+sPDQ/VlVfD/AAAA//+U9Uo47gmJhgAAAABJRU5ErkJggg==)\n",
        "\n",
        "it's possible to find an implementation in Scikits-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRpMPATbxaPh",
        "outputId": "cd69a5a8-04c1-45a0-d95c-c2e2bd70ae85"
      },
      "source": [
        "# Applying Non-Negative Matrix Factorization\n",
        " \n",
        "nmf = NMF(n_components=10, solver=\"mu\")\n",
        "W = nmf.fit_transform(X)\n",
        "H = nmf.components_\n",
        "\n",
        "for i, topic in enumerate(H):\n",
        "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1: way,people,time,ve,good,know,think,like,just,don\n",
            "Topic 2: info,help,looking,card,hi,know,advance,mail,does,thanks\n",
            "Topic 3: church,does,christians,christian,faith,believe,christ,bible,jesus,god\n",
            "Topic 4: league,win,hockey,play,players,season,year,games,team,game\n",
            "Topic 5: bus,floppy,card,controller,ide,hard,drives,disk,scsi,drive\n",
            "Topic 6: 20,price,condition,shipping,offer,space,10,sale,new,00\n",
            "Topic 7: running,problem,using,program,use,files,window,dos,file,windows\n",
            "Topic 8: nsa,law,algorithm,escrow,government,keys,clipper,encryption,chip,key\n",
            "Topic 9: state,war,turkish,armenians,government,armenian,jews,israeli,israel,people\n",
            "Topic 10: email,internet,pub,com,article,ftp,university,cs,soon,edu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiuf9Ok6JvkZ"
      },
      "source": [
        "When we decompose the representation into two matrices similar words will be close to each other. The word “eat” would be likely to appear in food-related articles, and therefore co-occur with words like “tasty” and “food”. Therefore, these words would probably be grouped together into a “food” component vector, and each article would have a certain weight of the “food” topic.\n",
        "Therefore, an NMF decomposition of the term-document matrix would yield components that could be considered “topics”, and decompose each document into a weighted sum of topics. This is called topic modeling and is an important application of NMF.\n",
        "\n",
        "This is another example where the underlying components (topics) and their weights should be non-negative.\n",
        "Another interesting property of NMF is that it naturally produces sparse representations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqyDItMgxSDe",
        "outputId": "af75067b-47b3-4ae3-85b3-2d5896a353a1"
      },
      "source": [
        "print(H[:10,:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.73056098e-17 1.36668423e-02 2.53748965e-05 1.10637295e-02\n",
            "  5.52709051e-07 1.44044380e-05 1.60522855e-08 7.40583315e-06\n",
            "  2.64988025e-68 3.33684954e-54]\n",
            " [1.99640281e-12 0.00000000e+00 1.58904902e-09 2.42163785e-12\n",
            "  2.63606445e-03 5.53213460e-04 4.91903700e-04 6.25551731e-10\n",
            "  3.37267223e-29 5.33354304e-36]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [8.00543659e-11 4.51766792e-02 1.74974107e-03 2.19629994e-03\n",
            "  2.12219682e-03 3.35820275e-06 2.38710570e-03 2.61996182e-04\n",
            "  1.11511012e-07 3.44085840e-07]\n",
            " [6.36113869e-13 4.59738488e-03 0.00000000e+00 9.30771870e-03\n",
            "  0.00000000e+00 0.00000000e+00 4.46894025e-03 0.00000000e+00\n",
            "  2.62850141e-09 5.88084949e-11]\n",
            " [9.96690595e-01 2.35187247e-01 8.04633131e-02 5.30344540e-02\n",
            "  3.72116368e-02 7.34792940e-02 4.60705088e-02 4.26739192e-02\n",
            "  4.64046591e-03 2.50667205e-03]\n",
            " [0.00000000e+00 0.00000000e+00 2.15656893e-02 0.00000000e+00\n",
            "  1.38907646e-02 4.50463058e-04 5.93005573e-03 5.99155855e-03\n",
            "  1.95843242e-11 6.35956219e-15]\n",
            " [0.00000000e+00 5.21102769e-13 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.59307979e-67 3.74536176e-42]\n",
            " [7.66840852e-24 1.70708603e-01 6.79215587e-12 7.08577468e-03\n",
            "  1.11295769e-17 5.48179290e-07 1.37556552e-18 2.60740435e-09\n",
            "  1.47018649e-40 6.33565809e-37]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.25389439e-02\n",
            "  1.89549644e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgv6e3KK0MPE",
        "outputId": "af790520-bf2d-4ff2-e3f3-2003f07e0121"
      },
      "source": [
        "print(W[:10,:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.19899200e-02 2.92240867e-02 0.00000000e+00 3.31929055e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [6.13370596e-03 2.98563482e-02 9.45242384e-09 5.59946663e-04\n",
            "  3.17299443e-02 8.05593970e-03 0.00000000e+00 5.17698394e-03\n",
            "  8.46700302e-08 6.79058186e-04]\n",
            " [6.51443869e-02 6.11518854e-02 0.00000000e+00 8.40811885e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09107226e-03\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [4.35713959e-03 2.75717939e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 2.27908873e-02 0.00000000e+00 8.82905660e-02\n",
            "  0.00000000e+00 2.39390397e-16]\n",
            " [3.43456763e-02 5.79639104e-04 3.06596547e-03 0.00000000e+00\n",
            "  0.00000000e+00 2.50565357e-02 1.05897780e-02 0.00000000e+00\n",
            "  0.00000000e+00 9.20000663e-03]\n",
            " [1.61820688e-02 0.00000000e+00 3.75697641e-03 0.00000000e+00\n",
            "  6.35541109e-11 3.95591922e-03 8.51394041e-03 0.00000000e+00\n",
            "  1.68173246e-02 0.00000000e+00]\n",
            " [7.83745392e-03 6.40798550e-02 3.48897365e-04 2.57943711e-03\n",
            "  0.00000000e+00 5.94191853e-03 0.00000000e+00 0.00000000e+00\n",
            "  2.04417457e-02 4.28413838e-03]\n",
            " [8.80170165e-06 5.05091173e-03 8.49038715e-04 2.15494860e-15\n",
            "  1.14057374e-01 0.00000000e+00 0.00000000e+00 2.43153416e-02\n",
            "  0.00000000e+00 9.98550854e-03]\n",
            " [7.00313519e-03 3.30413079e-02 4.39334333e-13 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 2.46311670e-02 5.54522626e-03\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.67880142e-02 1.54438717e-02 7.44503655e-04 0.00000000e+00\n",
            "  2.22228833e-02 0.00000000e+00 3.63156502e-02 0.00000000e+00\n",
            "  0.00000000e+00 1.04419323e-10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SiVjWX0LIfB"
      },
      "source": [
        "The we may want to impose stronger sparsity constraints or prevent the weights from becoming too large. To solve these problems, we can introduce L1 and L2 regularization losses on the weights of the matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_jXbRomSuAj"
      },
      "source": [
        "# LSA (Latent Semantic Analysis)\n",
        "\n",
        "\n",
        "Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix.\n",
        "\n",
        "Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix.\n",
        "\n",
        "The first step is generating our document-term matrix. Given m documents and n words in our vocabulary, we can construct an m × n matrix A in which each row represents a document and each column represents a word. In the simplest version of LSA, each entry can simply be a raw count of the number of times the j-th word appeared in the i-th document. In practice, however, raw counts do not work particularly well because they do not account for the significance of each word in the document. For example, the word “nuclear” probably informs us more about the topic(s) of a given document than the word “test.”\n",
        "\n",
        "Consequently, LSA models typically replace raw counts in the document-term matrix with a tf-idf score. Tf-idf, or term frequency-inverse document frequency, assigns a weight for term j in document i as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMEXr2Fa4mBv"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "  \n",
        "# tf-idf matrix: \n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "                             use_idf=True, \n",
        "                             smooth_idf=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRlLUqu-Srps"
      },
      "source": [
        "# SVD to reduce dimensionality: \n",
        "svd_model = TruncatedSVD(n_components=100,\n",
        "                         algorithm='randomized',\n",
        "                         n_iter=10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CswAPesk5YZW"
      },
      "source": [
        "# pipeline of tf-idf + SVD, fit to and applied to documents:\n",
        "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
        "                            ('svd', svd_model)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKoWOoPg5cmk"
      },
      "source": [
        "svd_matrix = svd_transformer.fit_transform(text_data)\n",
        "# svd_matrix can later be used to compare documents, compare words, or compare queries with documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BayLsrF85d6w",
        "outputId": "ff31ade2-a336-44ce-e0c4-341427a3b22f"
      },
      "source": [
        "svd_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10844544,  0.01526925, -0.03565005, ..., -0.02215143,\n",
              "        -0.01419781, -0.03082219],\n",
              "       [ 0.07258907,  0.06080842,  0.01094119, ...,  0.01814924,\n",
              "        -0.01513074, -0.00087727],\n",
              "       [ 0.21922509,  0.05713034,  0.00277719, ...,  0.00439044,\n",
              "        -0.04791822,  0.02120843],\n",
              "       ...,\n",
              "       [ 0.0536384 ,  0.03130991, -0.01026261, ...,  0.00100542,\n",
              "        -0.00196158,  0.00296829],\n",
              "       [ 0.07042151, -0.02059494, -0.00911099, ..., -0.02152662,\n",
              "         0.02946954, -0.0337031 ],\n",
              "       [ 0.05823994,  0.01816512, -0.03403748, ..., -0.01148088,\n",
              "         0.00450796,  0.0343873 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INpeFKQ_5m-q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}